{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Super Hexagon Proximal Policy Optimization (PPO)","text":"<p>Note: This is a fork of SuperHexagonAI, incorporating Proximal Policy Optimization (PPO) for reinforcement learning.</p>"},{"location":"#what-is-super-hexagon","title":"What is Super Hexagon?","text":"<p>Super Hexagon is a high-intensity arcade game where players control a small triangle orbiting a central hexagon, avoiding incoming obstacles. Renowned for its extreme difficulty, the game demands sharp reflexes, precision, and rapid pattern recognition to survive.</p> <p> <p></p> <p>The game features six progressively challenging levels. A level is considered completed when the player survives for 60 seconds. The difficulty ranges from HARD to the extreme HARDESTESTEST. It's both demoralizingly tough and insanely addictive.</p>"},{"location":"#what-is-our-project","title":"What is our Project?","text":"<p>Our project extends the SuperHexagonAI repository by integrating Proximal Policy Optimization (PPO) instead of the original Deep Q-Networks (DQN) approach. We implemented two versions of PPO: one using Stable-Baselines3 (SB3) for experimentation, and another custom implementation with Gymnasium and PyTorch for deeper control over the learning process. Additionally, we updated the original repository to Python 3.12, resolved CMake errors, and optimized the environment to work efficiently with direct memory access instead of processing image data.  </p>"},{"location":"#gymnasium-environment","title":"Gymnasium Environment","text":"<p>To train the AI effectively, we created a custom reinforcement learning environment for Super Hexagon using Gymnasium. Instead of using image data, we access the game's memory directly to construct a structured state representation of the game state: - Wall distances: A list of normalized distances to the nearest walls for each slot around the central hexagon. - Player orientation: The normalized sine of the player\u2019s angle, providing a continuous representation of rotation. - Exit direction flag: A binary indicator showing whether the next safe opening is to the left or right, crucial for fast-turn scenarios.  </p> <p>The action space is discrete and consists of three possible moves: stay in place, rotate left, or rotate right. The reward function is based on survival time, penalizing the agent upon failure and incorporating adjustments based on angular distance to the next gap.</p>"},{"location":"#what-we-achieved","title":"What We Achieved","text":"<p>Initially, our agent struggled to survive beyond five seconds, but through iterative improvements, it can now reach up to 30 seconds. The integration of the \"next opening\" direction flag proved particularly useful in handling fast-paced 180-degree turns.  </p> <p>Additionally, we tested imitation learning, using the DQN model as a teacher in the reward function. However, this approach did not lead to significant improvements.</p>"},{"location":"#find-your-way-around","title":"Find Your Way Around","text":"<p>Follow these links to learn more about the different aspects of our project:</p> <ul> <li>Getting Started: Instructions for installing and starting the project.</li> <li>Architecture: Description of the architecture for implementing Proximal Policy Optimization.</li> <li>Implementation: Detailed explanation of the implementation.</li> <li>Performance: Analysis of performance and comparison with existing implementations.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#introduction","title":"Introduction","text":"<p>The architecture uses memory manipulation to extract the game state directly from RAM, train an agent with Proximal Policy Optimization (PPO), and enable it to execute its own inputs in the game. This approach is based on the original memory manipulation from SuperHexagonAI. There are two trainers, one with Stable-baselines3 and one that uses PyTorch, which handle the training via a unified Gymnasium environment. The environment interacts with the game via a C++ interface and memory access.</p>"},{"location":"architecture/#diagram","title":"Diagram","text":""},{"location":"environment/","title":"Environment","text":"<p>The Gymnasium environment is presented below.</p>"},{"location":"environment/#action-space","title":"Action Space","text":"<p>Despite the game's difficulty, its input system is simple, with a one-dimensional action space offering three possible actions:</p> <ol> <li>Stay: Remain still. No action will be executed in the game.</li> <li>Left: Rotate counterclockwise. Will call <code>_left(True)</code> at the start of the frame and <code>_left(False)</code> at the end of the frame.</li> <li>Right: Rotate clockwise. Will call <code>_right(True)</code> at the start of the frame and <code>_right(False)</code> at the end of the frame.</li> </ol> <p>This defines the complete set of actions required to play the game. Notably, we introduced the \"Stay\" action, which refrains from pressing any input. While this is not an action a player could typically take, it represents the absence of movement, which can be strategically beneficial in certain situations.</p>"},{"location":"environment/#observation-space","title":"Observation Space","text":"<p>The observation space consists of several variables that provide insights into the current state of the game. It is limited to three main variables that utilize a total of 8 values:</p> <ul> <li>[0]: Normalized player angle:</li> </ul> <p>Calculates the angle of the player as a continuous variable, by converting it into a sinus function and scaling/offsetting it. This was done to combat the sudden jump from 360\u00b0 to 0\u00b0. We observed that the agent was \"confused\" by this sudden change.</p> <p>Returns:</p> Type Description <code>float</code> <p>Normalized player angle in space [0-1]</p> Source code in <code>environment.py</code> <pre><code>def _get_norm_player_angle(self):\n    \"\"\"\n    Calculates the angle of the player as a continuous variable, by converting it into a sinus function and scaling/offsetting it.\n    This was done to combat the sudden jump from 360\u00b0 to 0\u00b0. We observed that the agent was \"confused\" by this sudden change.\n\n    Returns:\n        (float): Normalized player angle in space [0-1]\n    \"\"\"\n    return 0.5 + 0.5 * math.sin(math.radians(self.env.get_triangle_angle()))\n</code></pre> <ul> <li>[1]: Direction indicator to next free slot: The image below shows an example scenario. The method retrieves all walls and finds the edges of the closest free slot in both directions. After calculating the delta angle of both edges in relation to the player, it returns the direction with the minimal value (For discrete value buckets, see the method definition below).  In this example, the resulting value would be 1.0 (Right).</li> </ul> <p>Calculates the closest direction to a free slot and encodes it into three buckets</p> <p>Returns:</p> Type Description <code>float</code> <p>direction indicator to closest free slot; 0.0: left, 1.0: right, 0.5: already in safe slot</p> Source code in <code>environment.py</code> <pre><code>def _get_direction_indicator(self):\n    \"\"\"\n    Calculates the closest direction to a free slot and encodes it into three buckets\n\n    Returns:\n        (float): direction indicator to closest free slot; 0.0: left, 1.0: right, 0.5: already in safe slot\n    \"\"\"\n    num_slots = self.env.get_num_slots()\n    wall_info = self._get_wall_distances()\n    wall_info = wall_info / wall_info.sum()\n    player_angle = self.env.get_triangle_angle()\n    player_slot = self.env.get_triangle_slot()\n    left_border = 0\n    right_border = 0\n    if wall_info[player_slot] &gt; wall_info.min():\n        return 0.5\n    for i in range(player_slot, player_slot + num_slots):\n        index = i % num_slots\n        if wall_info[index] &gt; wall_info.min():\n            left_border = index * (360 / num_slots)\n            break\n    for i in range(player_slot, player_slot - num_slots, -1):\n        index = i % num_slots\n        if wall_info[index] &gt; wall_info.min():\n            right_border = (index + 1) * (360 / num_slots)\n            break\n    left_distance = min(\n        abs(player_angle - left_border), 360 - abs(player_angle - left_border)\n    )\n    right_distance = min(\n        abs(player_angle - right_border), 360 - abs(player_angle - right_border)\n    )\n    return 0.0 if left_distance &lt; right_distance else 1.0\n</code></pre> <ul> <li>[2-7]: Normalized wall distances per slot:   ::: environment.SuperHexagonGymEnv._get_norm_wall_distances</li> </ul> <p>These values are calculated at each step in the <code>_get_state()</code> method and are used as inputs for the model:</p> <ul> <li> <p>Gathers all observations in a numpy array</p> <p>Example output: [0.80090751, 0.5, 0.0967198, 0.3832005, 0.02007971, 0.3832005, 0.0967198, 0.02007971]</p> <p>Returns:</p> Type Description <code>ndarray[float]</code> <p>A one-dimensional numpy array representing all observations</p> Source code in <code>environment.py</code> <pre><code>def _get_state(self):\n    \"\"\"\n    Gathers all observations in a numpy array\n\n    Example output: [0.80090751, 0.5, 0.0967198, 0.3832005, 0.02007971, 0.3832005, 0.0967198, 0.02007971]\n\n    Returns:\n        (ndarray[float]): A one-dimensional numpy array representing all observations\n    \"\"\"\n    return np.concatenate(\n        [\n            np.array([self._get_norm_player_angle()]),\n            np.array([self._get_direction_indicator()]),\n            self._get_norm_wall_distances(),\n        ]\n    )\n</code></pre> </li> </ul>"},{"location":"environment/#reward-calculation","title":"Reward Calculation","text":"<p>The reward system was refined multiple times throughout the agent's development. It combines both rewards (positive values) and penalties (negative values) to provide more direct feedback. We distinguish between two cases:</p> <ul> <li> <p>Agent is in a free slot</p> </li> <li> <p><code>1.0</code>: If the \"Stay\" action is selected.</p> </li> <li><code>0.7</code>: If any action other than \"Stay\" is selected.</li> <li> <p>This will always result in a positive reward, with the value depending on the action chosen. This approach was adopted to account for the player \"jittering\" within a free slot, even though it may remain in a safe position. Such jittering sometimes led to a gameover when the agent hit the wall due to slight movement near the slot edges.</p> </li> <li> <p>Agent is in an occupied slot</p> </li> <li><code>[(-1)-0]</code>: A continuous negative value, which depends on the distance to the closest free slot. As the agent moves further away from a free slot, the value decreases (becoming closer to -1). Conversely, the value increases as the agent approaches a free slot, reaching 0 and eventually transitioning to a reward.</li> <li>This was implemented to give the agent a stronger incentive to reach a free slot as quickly as possible, avoiding wasting time in an occupied slot.</li> </ul> <p>\"Free slot\" is defined as a slot where, if the player remains in it, they will transition to the next wave of walls without resulting in a gameover.</p> <ul> <li> <p>Calculates the reward that the agent gets for the current state and the action it took.</p> <p>Parameters:</p> Name Type Description Default <code>done</code> <code>bool</code> <p>If True, gives out max penalty because the agent hit a wall; otherwise normal calculated reward.</p> required <code>action</code> <code>int</code> <p>Action that the agent took for this step. Used for some reward dependencies.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A positive reward value or a negative penalty value</p> Source code in <code>environment.py</code> <pre><code>def _get_reward(self, done, action):\n    \"\"\"\n    Calculates the reward that the agent gets for the current state and the action it took.\n\n    Args:\n        done (bool): If True, gives out max penalty because the agent hit a wall; otherwise normal calculated reward.\n        action (int): Action that the agent took for this step. Used for some reward dependencies.\n\n    Returns:\n        (float): A positive reward value or a negative penalty value\n    \"\"\"\n    debug = self.env.debug_mode\n    if done:\n        if debug:\n            print(\"Game over! Return -10.0\")\n        return -10.0\n    reward = 1\n\n    distance = self.get_exit_distance()\n    distance_factor = distance / 360\n    if distance_factor == 0:\n        reward *= 1 if action == 0 else 0.7\n    else:\n        reward *= -distance_factor\n    if debug:\n        print(\"-------------------------------------------------------\")\n        print(f\"Player Angle: {self.env.get_triangle_angle()}\")\n        print(f\"Player Slot: {self.env.get_triangle_slot()}\")\n        print(f\"Distance: {distance} and Factor {distance_factor}\")\n        print(f\"Action Taken: {action}\")\n        print(f\"State: {self._get_state()}\")\n        print(f\"Calculated Reward: {reward}\")\n        print(\"-------------------------------------------------------\\n\")\n    return float(reward)\n</code></pre> </li> </ul>"},{"location":"environment/#debugging-controls","title":"Debugging Controls","text":"<p>Due to the sped-up nature of the game, debugging became quite challenging. To address this, we implemented key combinations that allow us to interact with the game while it is running. Using the <code>keyboard</code> library, we listened for key presses and created the following features:</p> <ul> <li>Freezing the entire process:</li> <li><code>Ctrl + Space</code></li> <li>Slowing down time:</li> <li><code>Ctrl + Alt</code></li> <li>Toggling Debug logs:</li> <li><code>Ctrl + D</code></li> </ul> <p>These controls enabled us to inspect specific scenarios in greater detail, gaining deeper insights into the agent's behavior and its internal values. With these debugging tools, we were able to identify the previously discussed player rotation anomaly and also detect some challenging layouts for the agent.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, ensure you have the following installed on your system:</p> <ul> <li>Visual Studio Community Edition 2022</li> <li>Python 3.12 (not the Microsoft Store version)</li> <li>CMake</li> <li>Git</li> <li>Steam</li> <li>Pre-Neo Beta version of Super Hexagon</li> </ul>"},{"location":"getting-started/#step-1-install-c-build-tools-cmake-and-python","title":"Step 1: Install C++ Build Tools, CMake, and Python","text":"<p>Please follow these steps to install Visual Studio Community Edition 2022, CMake, and Python 3.12:</p> <ol> <li> <p>Download and run the Visual Studio Installer.</p> <ul> <li>During installation, select these components:<ul> <li>MSVC C++ x64/x86 Build Tools</li> <li>C++ CMake Tools for Windows</li> <li>Windows 11 SDK</li> </ul> </li> </ul> </li> <li> <p>Download and install CMake.</p> <ul> <li>Ensure CMake is added to your system PATH.</li> </ul> </li> <li> <p>Download and install Python 3.12.</p> <ul> <li>Ensure Python is added to your system PATH.</li> <li>The Microsoft Store version doesn't work because it blocks system-wide package installation needed for the RLHook library.</li> </ul> </li> </ol>"},{"location":"getting-started/#step-2-install-python-dependencies","title":"Step 2: Install Python Dependencies","text":"<p>Please install the required Python packages using the provided <code>requirements.txt</code> file:</p> <pre><code>python -m pip install -r requirements.txt\n</code></pre> <p>Ensure your <code>requirements.txt</code> file contains the following:</p> <pre><code>ghapi==1.0.6\ngymnasium==1.0.0\nkeyboard==0.13.5\nmatplotlib==3.10.1\nnox==2025.2.9\nnumpy==2.2.4\nopencv_python==4.11.0.86\npytest==8.3.5\nrich==13.9.4\nsetuptools==76.1.0\nstable_baselines3==2.5.0\ntorch==2.6.0\n</code></pre>"},{"location":"getting-started/#step-3-setup-super-hexagon","title":"Step 3: Setup Super Hexagon","text":"<p>To use this setup, you need Super Hexagon in the Pre-Neo Steam Beta version. Ensure the following settings are applied:</p> <ul> <li>Run as Administrator:</li> <li>Right-click <code>superhexagon.exe</code>, select <code>Properties</code>, go to the <code>Compatibility</code> tab, and check \"Run this program as an administrator\".</li> <li>Windowed Mode &amp; VSync:</li> <li>The game must be in windowed mode, and VSync should be disabled.</li> </ul> <p>Additionally, ensure that both the Python process and the game process are run with admin privileges.</p>"},{"location":"getting-started/#step-4-clone-the-repository","title":"Step 4: Clone the Repository","text":"<p>To download the necessary files, run:</p> <pre><code>git clone https://github.com/Stinktopf/SuperHexagonAI.git\ncd SuperHexagonAI\n</code></pre>"},{"location":"getting-started/#step-5-compile-the-dll-helper-executable","title":"Step 5: Compile the DLL &amp; Helper Executable","text":"<p>To compile the necessary binaries, execute:</p> <pre><code>cd RLHookLib\npython compile_additional_binaries.py\n</code></pre>"},{"location":"getting-started/#step-6-install-the-library","title":"Step 6: Install the Library","text":"<p>Install the library globally using pip:</p> <pre><code>pip install .\n</code></pre> <p>Go to the repository root:</p> <pre><code>cd ..\n</code></pre>"},{"location":"getting-started/#step-7-run-the-trainer","title":"Step 7: Run the Trainer","text":"<p>Execute a trainer in an administrator command line:</p> <pre><code>python trainer_PPO_GYM_SB3.py\n</code></pre> <p>or</p> <pre><code>python trainer_PPO_GYM.py\n</code></pre> <p>or</p> <pre><code>python trainer_DQN_GYM_SB3.py\n</code></pre>"},{"location":"interface/","title":"Superhexagon Interface","text":"<p>To interface with the game's memory, we utilized the implementation of SuperHexagonAI. This project provides a Python wrapper for the C++ memory hook developed in super-hexagon-ai. With this wrapper, we accessed the game's memory, enabling features like:</p> <ul> <li>Starting the game</li> <li>Speeding up the game</li> <li>Freezing the game</li> <li>Stepping one frame at a time</li> <li>Selecting a level</li> <li>Restarting a level</li> </ul> <p>With the help of the wrapper and provided functions, it is also possible to carry out memory manipulations on the game</p> <ul> <li> <p>_left</p> <p>Simulates pressing the Left key in the game.</p> <p>Parameters:</p> Name Type Description Default <code>down</code> <code>bool</code> <p>If True, presses the key (writes 1 to memory); otherwise, releases it (writes 0).</p> required Source code in <code>superhexagon.py</code> <pre><code>def _left(self, down):\n    \"\"\"\n    **_left**\n\n    Simulates pressing the **Left** key in the game.\n\n    Args:\n        down (bool): If True, presses the key (writes 1 to memory); otherwise, releases it (writes 0).\n    \"\"\"\n    self.game.write_byte(\n        \"superhexagon.exe\", [0x00294B00, 0x428BD], 1 if down else 0\n    )\n</code></pre> </li> <li> <p>_right</p> <p>Simulates pressing the Right key in the game.</p> <p>Parameters:</p> Name Type Description Default <code>down</code> <code>bool</code> <p>If True, presses the key (writes 1 to memory); otherwise, releases it (writes 0).</p> required Source code in <code>superhexagon.py</code> <pre><code>def _right(self, down):\n    \"\"\"\n    **_right**\n\n    Simulates pressing the **Right** key in the game.\n\n    Args:\n        down (bool): If True, presses the key (writes 1 to memory); otherwise, releases it (writes 0).\n    \"\"\"\n    self.game.write_byte(\n        \"superhexagon.exe\", [0x00294B00, 0x428C0], 1 if down else 0\n    )\n</code></pre> </li> <li> <p>get_triangle_angle</p> <p>Retrieves the player's rotation angle in degrees.</p> <p>Returns:</p> Type Description <code>int</code> <p>The rotation angle of the player (0-360 degrees).</p> Source code in <code>superhexagon.py</code> <pre><code>def get_triangle_angle(self):\n    \"\"\"\n    **get_triangle_angle**\n\n    Retrieves the player's rotation angle in degrees.\n\n    Returns:\n        (int): The rotation angle of the player (0-360 degrees).\n    \"\"\"\n    return self.game.read_dword(\"superhexagon.exe\", [0x00294B00, 0x2958])\n</code></pre> </li> </ul> <p>These functions provided a solid foundation but had limitations requiring deeper insights into the game's state. To overcome them, we extended the wrapper by identifying additional memory locations from the C++ implementation, adding the following methods:</p> <ul> <li> <p>get_num_slots</p> <p>Retrieves the total number of slots in the game.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of available slots.</p> Source code in <code>superhexagon.py</code> <pre><code>def get_num_slots(self):\n    \"\"\"\n    **get_num_slots**\n\n    Retrieves the total number of slots in the game.\n\n    Returns:\n        (int): The number of available slots.\n    \"\"\"\n    return self.game.read_dword(\"superhexagon.exe\", [0x00294B00, 0x1BC])\n</code></pre> </li> <li> <p>get_num_walls</p> <p>Retrieves the total number of walls currently present in the game.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of walls.</p> Source code in <code>superhexagon.py</code> <pre><code>def get_num_walls(self):\n    \"\"\"\n    **get_num_walls**\n\n    Retrieves the total number of walls currently present in the game.\n\n    Returns:\n        (int): The number of walls.\n    \"\"\"\n    return self.game.read_dword(\"superhexagon.exe\", [0x00294B00, 0x2930])\n</code></pre> </li> <li> <p>get_triangle_slot</p> <p>Determines the player's current slot based on their rotation angle.</p> <p>Returns:</p> Type Description <code>int</code> <p>The slot index the player is currently occupying.</p> Source code in <code>superhexagon.py</code> <pre><code>def get_triangle_slot(self):\n    \"\"\"\n    **get_triangle_slot**\n\n    Determines the player's current slot based on their rotation angle.\n\n    Returns:\n        (int): The slot index the player is currently occupying.\n    \"\"\"\n    return floor(self.get_triangle_angle() / 360.0 * self.get_num_slots())\n</code></pre> </li> <li> <p>get_walls</p> <p>Retrieves information about all active walls in the game.</p> <p>Returns:</p> Type Description <code>list[Wall]</code> <p>A list of Wall objects, each representing a wall's properties.</p> Source code in <code>superhexagon.py</code> <pre><code>def get_walls(self):\n    \"\"\"\n    **get_walls**\n\n    Retrieves information about all active walls in the game.\n\n    Returns:\n        (list[Wall]): A list of Wall objects, each representing a wall's properties.\n    \"\"\"\n    num_walls = self.get_num_walls()\n    walls = []\n\n    for i in range(num_walls):\n        base_addr = 0x220 + i * 20  # Each Wall struct is 20 bytes (0x14)\n        slot = self.game.read_dword(\"superhexagon.exe\", [0x00294B00, base_addr])\n        distance = self.game.read_dword(\n            \"superhexagon.exe\", [0x00294B00, base_addr + 4]\n        )\n        enabled = self.game.read_byte(\n            \"superhexagon.exe\", [0x00294B00, base_addr + 8]\n        )\n        fill = []  # Padding to align dwords\n        for j in range(3):\n            fill.append(\n                self.game.read_byte(\n                    \"superhexagon.exe\", [0x00294B00, base_addr + 9 + j]\n                )\n            )\n\n        unk2 = self.game.read_dword(\n            \"superhexagon.exe\", [0x00294B00, base_addr + 12]\n        )\n        unk3 = self.game.read_dword(\n            \"superhexagon.exe\", [0x00294B00, base_addr + 16]\n        )\n\n        walls.append(\n            Wall(\n                slot=slot,\n                distance=distance,\n                enabled=enabled,\n                fill=fill,\n                unk2=unk2,\n                unk3=unk3,\n            )\n        )\n\n    return walls\n</code></pre> </li> </ul> <p>These enhancements enabled us to gather all necessary data for observations and reward calculation.</p>"},{"location":"legal/","title":"Legal","text":""},{"location":"legal/#privacy-policy","title":"Privacy Policy","text":""},{"location":"legal/#general-information","title":"General Information","text":"<p>This privacy policy provides an overview of what happens to your personal data when you visit this website. Personal data refers to any information that can identify you personally.</p> <p>Detailed information on data protection can be found in the privacy policy listed below.</p>"},{"location":"legal/#who-is-responsible-for-data-collection","title":"Who is responsible for data collection?","text":"<p>The data processing on this website is carried out by the website operator:</p> <p>Lucas Immanuel Nickel Im Kornfeld 5, 36124 Eichenzell, Germany Email: lucas-immanuel.nickel[at]outlook.de  </p>"},{"location":"legal/#data-collection","title":"Data Collection","text":"<p>Some data is collected automatically when visiting the website, such as:</p> <ul> <li>IP address</li> <li>Browser type and version</li> <li>Operating system</li> <li>Date and time of access</li> </ul> <p>These data are collected to ensure the proper functioning and security of the website.</p> <p>This website does not use cookies or tracking technologies.</p>"},{"location":"legal/#hosting","title":"Hosting","text":"<p>This website is hosted externally. The hosting provider is:</p> <p>GitHub, Inc. 88 Colin P Kelly Jr St, San Francisco, CA 94107, USA  </p> <p>We inform you that using GitHub as a hosting provider may result in data transfer to the United States. GitHub is certified under the EU-U.S. Data Privacy Framework (DPF), ensuring an adequate level of data protection according to EU standards.</p> <p>For more information, please refer to GitHub's privacy policy: GitHub Privacy Policy.</p>"},{"location":"legal/#legal-basis-for-processing","title":"Legal Basis for Processing","text":"<p>Data processing is based on:</p> <ul> <li>Art. 6(1)(f) GDPR \u2013 Legitimate interest in ensuring website functionality and security.</li> </ul>"},{"location":"legal/#your-rights","title":"Your Rights","text":"<p>You have the right to request information about your stored personal data, its origin, recipients, and purpose of processing. You also have the right to request correction or deletion of this data.</p> <p>Additionally, you have the right to lodge a complaint with the competent data protection authority. In Germany, this is the Federal Commissioner for Data Protection and Freedom of Information (BfDI).</p>"},{"location":"legal/#data-security","title":"Data Security","text":"<p>This site uses SSL/TLS encryption for secure data transmission. An encrypted connection can be recognized by the \"https://\" in the address bar and the lock icon in the browser.</p>"},{"location":"legal/#changes-to-this-privacy-policy","title":"Changes to this Privacy Policy","text":"<p>This privacy policy may be updated to comply with legal requirements or reflect changes in data processing.</p>"},{"location":"legal/#source","title":"Source","text":"<p>This privacy policy is based on a template provided by eRecht24.</p>"},{"location":"legal/#imprint","title":"Imprint","text":"<p>Responsible for content according to \u00a7 5 TMG:  </p> <p>Lucas Immanuel Nickel Im Kornfeld 5, 36124 Eichenzell, Germany Email: lucas-immanuel.nickel[at]outlook.de  </p>"},{"location":"no_sb3/","title":"Training witout Stable-Baselines3","text":"<p>This section presents a PPO implementation without Stable-Baselines3, using the same hyperparameter configuration.</p>"},{"location":"no_sb3/#network-architecture","title":"Network Architecture","text":"<p>The PPO agent uses an Actor-Critic network with shared feature extraction layers. The model consists of:</p> <ul> <li>Shared Layers: Extract general representations from observations.</li> <li>Policy Head: Outputs logits for action selection.</li> <li>Value Head: Predicts state value for advantage estimation.</li> </ul> <p>The model is implemented using the ActorCritic class:</p> <ul> <li> <p>__init__</p> Source code in <code>trainer_PPO_GYM.py</code> <pre><code>def __init__(self, input_dim, num_actions, net_arch):\n    r\"\"\"**\\_\\_init\\_\\_**\"\"\"\n    super(ActorCritic, self).__init__()\n    layers = []\n    last_dim = input_dim\n    for hidden_dim in net_arch:\n        layers.append(nn.Linear(last_dim, hidden_dim))\n        layers.append(nn.ReLU())\n        last_dim = hidden_dim\n    self.shared = nn.Sequential(*layers)\n    self.policy_head = nn.Linear(last_dim, num_actions)\n    self.value_head = nn.Linear(last_dim, 1)\n</code></pre> </li> <li> <p>forward</p> Source code in <code>trainer_PPO_GYM.py</code> <pre><code>def forward(self, x):\n    \"\"\"**forward**\"\"\"\n    x = self.shared(x)\n    logits = self.policy_head(x)\n    value = self.value_head(x)\n    return logits, value\n</code></pre> </li> </ul>"},{"location":"no_sb3/#rollout-collection","title":"Rollout Collection","text":"<p>During rollouts, the agent interacts with the environment and collects transitions, consisting of:</p> <ul> <li>State: The current observation.</li> <li>Action: The chosen action.</li> <li>Reward: The received reward.</li> <li>Next State: The next observation after the action.</li> <li>Done Flag: Indicates whether the episode has ended.</li> </ul>"},{"location":"no_sb3/#observation-forward-pass","title":"Observation &amp; Forward Pass","text":"<p>The Actor-Critic network processes the state tensor, outputting action logits and a value estimate:</p> <pre><code>obs_tensor = torch.FloatTensor(obs).to(self.device).unsqueeze(0)\nlogits, value = self.model(obs_tensor)\n</code></pre>"},{"location":"no_sb3/#action-selection-environment-step","title":"Action Selection &amp; Environment Step","text":"<p>Actions are sampled from a categorical distribution over the logits. The selected action is executed in the environment:</p> <pre><code>dist = Categorical(torch.softmax(logits, dim=-1))\naction = dist.sample()\nobs, reward, done, _, _ = self.env.step(action.item())\n</code></pre>"},{"location":"no_sb3/#entropy-coefficient-update","title":"Entropy Coefficient Update","text":"<p>To balance exploration and exploitation, the entropy coefficient (<code>ent_coef</code>) is annealed over the course of the training:</p> Source code in <code>trainer_PPO_GYM.py</code> <pre><code>def update_ent_coef(self, current_step):\n    progress = 1.0 - (current_step / self.total_timesteps)\n    self.ent_coef = self.final_ent_coef + progress * (\n        self.initial_ent_coef - self.final_ent_coef\n    )\n</code></pre>"},{"location":"no_sb3/#generalized-advantage-estimation-gae","title":"Generalized Advantage Estimation (GAE)","text":"<p>After collecting a batch of transitions, GAE computes advantages, reducing variance while keeping the estimator unbiased.</p> <p>The advantage function is computed as:</p>  \\delta_t = r_t + \\gamma\\,(1-d_{t+1})\\,V(s_{t+1}) - V(s_t)   A_t = \\delta_t + \\gamma\\,\\lambda\\,(1-d_{t+1})\\,A_{t+1}   R_t = A_t + V(s_t)  <p>This is shown in the following code:</p> Source code in <code>trainer_PPO_GYM.py</code> <pre><code>def compute_gae(self, rewards, values, dones, last_value):\n    advantages = np.zeros_like(rewards)\n    T = len(rewards)\n    start = 0\n    while start &lt; T:\n        end = start\n\n        while end &lt; T:\n            end += 1\n            if dones[end - 1] == 1:\n                break\n\n        seg_last_value = 0.0 if dones[end - 1] == 1 else last_value\n\n        lastgaelam = 0\n        for t in reversed(range(start, end)):\n            if t == end - 1:\n                next_non_terminal = 0.0 if dones[t] == 1 else 1.0\n                next_value = seg_last_value\n            else:\n                next_non_terminal = 1.0 - dones[t + 1]\n                next_value = values[t + 1]\n            delta = rewards[t] + self.gamma * next_value * next_non_terminal - values[t]\n            lastgaelam = delta + self.gamma * self.gae_lambda * next_non_terminal * lastgaelam\n            advantages[t] = lastgaelam\n        start = end\n    returns = advantages + values\n    return advantages, returns\n</code></pre> <p>Here is the revised version with the missing mathematical formulas for the Mini-Batch Updates, now in English:</p>"},{"location":"no_sb3/#mini-batch-updates","title":"Mini-Batch Updates","text":"<p>Once the advantages are computed, the data is shuffled and divided into mini-batches for policy and value function updates.</p>"},{"location":"no_sb3/#forward-pass-probability-ratio","title":"Forward Pass &amp; Probability Ratio","text":"<p>The PPO algorithm relies on the ratio between the new policy and the old policy to ensure that updates are constrained:</p>  r_t = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}  <p>This ratio is computed in the code as follows:</p> <pre><code>logits, values_pred = self.model(mb_obs)\nratio = torch.exp(new_logprobs - mb_old_logprobs)\n</code></pre>"},{"location":"no_sb3/#clipped-policy-loss","title":"Clipped Policy Loss","text":"<p>The policy loss uses the Clipped Surrogate Objective to keep updates close to the previous policy:</p>  L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t A_t, \\text{clip}(r_t, 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]  <p>In code:</p> <pre><code>surr1 = ratio * mb_advantages\nsurr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * mb_advantages\npolicy_loss = -torch.min(surr1, surr2).mean()\n</code></pre>"},{"location":"no_sb3/#value-loss","title":"Value Loss","text":"<p>The value function is optimized using a Mean Squared Error (MSE) loss:</p>  L^{\\text{VF}}(\\theta) = \\mathbb{E}_t \\left[ (V_\\theta(s_t) - R_t)^2 \\right]  <p>In code:</p> <pre><code>value_loss = ((mb_returns - values_pred.squeeze()) ** 2).mean()\n</code></pre>"},{"location":"no_sb3/#entropy-bonus","title":"Entropy Bonus","text":"<p>The entropy is added to encourage exploration:</p>  L^{\\text{ENT}}(\\theta) = \\mathbb{E}_t \\left[ H(\\pi_\\theta) \\right]  <p>In code:</p> <pre><code>entropy = dist.entropy().mean()\n</code></pre>"},{"location":"no_sb3/#final-loss-and-parameter-update","title":"Final Loss and Parameter Update","text":"<p>The total PPO loss function is:</p>  L(\\theta) = L^{\\text{CLIP}}(\\theta) + c_1 L^{\\text{VF}}(\\theta) - c_2 L^{\\text{ENT}}(\\theta)  <p>In code:</p> <pre><code>loss = policy_loss + self.vf_coef * value_loss - self.ent_coef * entropy\nself.optimizer.zero_grad()\nloss.backward()\nself.optimizer.step()\n</code></pre>"},{"location":"performance/","title":"Performance","text":"<p>The performance of our approach is outlined below.</p>"},{"location":"performance/#proximal-policy-optimization-with-stable-baselines3","title":"Proximal Policy Optimization with Stable-Baselines3","text":""},{"location":"performance/#proximal-policy-optimization-without-stable-baselines3","title":"Proximal Policy Optimization without Stable-Baselines3","text":""},{"location":"performance/#deep-q-network","title":"Deep Q Network","text":""},{"location":"sb3/","title":"Training with Stable-Baselines3","text":"<p>The following section presents the implementation using Stable-Baselines3.</p>"},{"location":"sb3/#setup","title":"Setup","text":"<p>Stable-Baselines3 provides a direct implementation of the PPO algorithm, which we use for training our RL agent. By leveraging this framework, we can seamlessly integrate our Gymnasium environment and train the agent.</p> <p>The usage is as follows: <pre><code>env = SuperHexagonGymEnv()\ntotal_timesteps = 1_000_000\nmodel = PPO(\n    policy=\"MlpPolicy\",\n    env=env,\n    verbose=1,\n    device=\"cpu\",\n    n_steps=4096,\n    batch_size=32,\n    learning_rate=2e-4,\n    gamma=0.999,\n    gae_lambda=0.95,\n    ent_coef=0.01,\n    vf_coef=0.6,\n    max_grad_norm=0.5,\n    policy_kwargs=dict(net_arch=[512, 512, 256]),\n)\nent_coef_scheduler = EntropyCoefficientScheduler(\n    total_timesteps=total_timesteps, initial_ent_coef=0.1, final_ent_coef=0.005\n)\nmodel.learn(total_timesteps=total_timesteps, callback=ent_coef_scheduler)\n</code></pre></p>"},{"location":"sb3/#settings","title":"Settings","text":"<p>The PPO agent is configured with:</p> <ul> <li>Policy: A multi-layer perceptron is used.</li> <li>Learning Rate: <code>2e-4</code> - Stabilizes learning.</li> <li>Gamma: <code>0.999</code> - Prioritizes long-term survival over short-term gains.</li> <li>Entropy Coefficient: <code>0.01 \u2192 0.005</code> - Encourages early exploration, then stabilizes decision-making.</li> <li>Policy Network Architecture: <code>[512, 512, 256]</code> - Optimised by trial and error.</li> </ul>"},{"location":"sb3/#entropy-scheduling","title":"Entropy Scheduling","text":"<p>To ensure a balance between exploration and exploitation, we linearly scale the entropy coefficient during training. </p> <p>This is achieved using the <code>EntropyCoefficientScheduler</code> callback:</p> <ul> <li> <p>__init__</p> Source code in <code>trainer_PPO_GYM_SB3.py</code> <pre><code>def __init__(\n    self,\n    total_timesteps: int,\n    initial_ent_coef: float,\n    final_ent_coef: float,\n    verbose: int = 0,\n):\n    r\"\"\"**\\_\\_init\\_\\_**\"\"\"\n    super().__init__(verbose)\n    self.total_timesteps = total_timesteps\n    self.initial_ent_coef = initial_ent_coef\n    self.final_ent_coef = final_ent_coef\n</code></pre> </li> <li> <p>_on_step</p> Source code in <code>trainer_PPO_GYM_SB3.py</code> <pre><code>def _on_step(self) -&gt; bool:\n    \"\"\"**_on_step**\"\"\"\n    progress = 1.0 - (self.num_timesteps / self.total_timesteps)\n    new_ent_coef = self.final_ent_coef + progress * (\n        self.initial_ent_coef - self.final_ent_coef\n    )\n    self.model.ent_coef = new_ent_coef\n    return True\n</code></pre> </li> </ul>"}]}